---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
editor:
  markdown: 
    wrap: 72
---

![](images/nypd3.jpeg)

This comes from the file `analysis.qmd`.

We describe here our detailed data analysis. This page will provide an
overview of what questions you addressed, illustrations of relevant
aspects of the data with tables and figures, and a statistical model
that attempts to answer part of the question. You'll also reflect on
next steps and further analysis.

The audience for this page is someone like your class mates, so you can
expect that they have some level of statistical and quantitative
sophistication and understand ideas like linear and logistic regression,
coefficients, confidence intervals, overfitting, etc.

While the exact number of figures and tables will vary and depend on
your analysis, you should target around 5 to 6. An overly long analysis
could lead to losing points. If you want you can link back to your blog
posts or create separate pages with more details.

The style of this paper should aim to be that of an academic paper. I
don't expect this to be of publication quality but you should keep that
aim in mind. Avoid using "we" too frequently, for example "We also found
that ...". Describe your methodology and your findings but don't
describe your whole process.

### Example of loading data

The code below shows an example of loading the loan refusal data set
(which you should delete at some point).

## Note on Attribution

In general, you should try to provide links to relevant resources,
especially those that helped you. You don't have to link to every
StackOverflow post you used but if there are explainers on aspects of
the data or specific models that you found helpful, try to link to
those. Also, try to link to other sources that might support (or refute)
your analysis. These can just be regular hyperlinks. You don't need a
formal citation.

If you are directly quoting from a source, please make that clear. You
can show quotes using `>` like this

```         
> To be or not to be.
```

> To be or not to be.

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Introduce what motivates your Data Analysis (DA)
    -   Which variables and relationships are you most interested in?
    -   What questions are you interested in answering?
    -   Provide context for the rest of the page. This will include
        figures/tables that illustrate aspects of the data of your
        question.
-   Modeling and Inference
    -   The page will include some kind of formal statistical model.
        This could be a linear regression, logistic regression, or
        another modeling framework.
    -   Explain the ideas and techniques you used to choose the
        predictors for your model. (Think about including interaction
        terms and other transformations of your variables.)
    -   Describe the results of your modelling and make sure to give a
        sense of the uncertainty in your estimates and conclusions.
-   Explain the flaws and limitations of your analysis
    -   Are there some assumptions that you needed to make that might
        not hold? Is there other data that would help to answer your
        questions?
-   Clarity Figures
    -   Are your figures/tables/results easy to read, informative,
        without problems like overplotting, hard-to-read labels, etc?
    -   Each figure should provide a key insight. Too many figures or
        other data summaries can detract from this. (While not a hard
        limit, around 5 total figures is probably a good target.)
    -   Default `lm` output and plots are typically not acceptable.
-   Clarity of Explanations
    -   How well do you explain each figure/result?
    -   Do you provide interpretations that suggest further analysis or
        explanations for observed phenomenon?
-   Organization and cleanliness.
    -   Make sure to remove excessive warnings, hide most or all code,
        organize with sections or multiple pages, use bullets, etc.
    -   This page should be self-contained, i.e. provide a description
        of the relevant data.

```{r include=F}
library(dplyr)
library(tidyverse) 
library(ggplot2)
library(viridis)
library(dplyr)
library(kableExtra)
library(corrplot)
library(reshape2) # for melting the correlation matrix
library(pheatmap)
library(gridExtra)
library(data.table)
library(sp)
library(sf)
library(MLmetrics)
library(MASS)
library(rpart)
library(rpart.plot)
ds <- read_rds("dataset/load_and_clean_data.rds") 
```

## Introduction

From our data page, we know that the Level 3 Assault is the most common
cases in the New York city, so we decide to locate our target on the
Demographic factor and Environmental factor that would effect the
incidents of Level 3 Assault. Through this analysis, We could uncover
correlations that inform targeted interventions, enhance policy-making,
and improve resource allocation. By identifying specific risk factors
related to demographics and environmental conditions, the analysis aims
to guide more effective prevention strategies, optimize urban planning,
and strengthen community engagement and support.

Our team will focus on the analysis of following variables in the
dataset:

-   Demographic factor : Age, Race, Borough

-   Environmental factor: Temperature,weather condition,wind speed

## Demographic factor

### Age

```{r echo=FALSE, message=FALSE, warning= FALSE}
assault_data <- ds %>%
  filter(grepl("ASSAULT 3 & RELATED OFFENSES", OFNS_DESC)) # Update grepl argument based on how Assault 3 is written in OFNS_DESC

# Create a summary of Assault 3 incidents by age group
assault_age_summary <- assault_data %>%
  group_by(AGE_GROUP) %>%
  summarize(Incident_Count = n())

# Create a bar plot for Assault 3 incidents by age group
ggplot(assault_age_summary, aes(x = AGE_GROUP, y = Incident_Count, fill = AGE_GROUP)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Fig1: Assault 3 Incidents count by Age Group",
       x = "Age Group",
       y = "Incident Count",
       fill = "Age Group") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

From this bar chart, we could know that the distribution of Assault 3
incidents across different age groups in New York City. It highlights
that the age group 25-44 has the highest incidence count, significantly
surpassing other age groups, with the lowest counts observed in the
under-18 and 65+ age brackets. Next we are going to move to gender and
race's relationship with Assault 3 crime rate.

### Gender and Race

```{r echo=FALSE, message=FALSE, warning= FALSE}
assault_3_by_race_gender <- ds %>%
  filter(grepl("ASSAULT 3 & RELATED OFFENSES", OFNS_DESC)) %>%
  count(PERP_RACE, PERP_SEX) %>%
  arrange(n)  

assault_3_by_race_gender |>
  ggplot(aes(x = reorder(PERP_RACE, n), y = n, fill = PERP_SEX)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(x = 'Race', y = 'Incidents Count', title = 'Fig2: Assault 3 Incidents count by Race and Gender', fill = 'Gender') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set1") 
```

From this graph we could directly find out that males always have higher
number of incidents than female. Also, the highest number of incidents
is associated with Black males, followed by White Hispanic males.

However, it have some **limitations** of this graph. The graph's
representation of Assault 3 incidents by race and gender may be skewed
by reporting biases and systemic disparities in arrest records, as well
as not accounting for the actual demographic distribution of New York
City's population.

### Borough

```{r echo=FALSE, message=FALSE, warning= FALSE}
library(ggplot2)
library(dplyr)

assault_3_by_borough <- ds %>%
  filter(grepl("ASSAULT 3 & RELATED OFFENSES", OFNS_DESC)) %>%
  group_by(ARREST_BORO) %>%
  summarize(Incident_Count = n(), .groups = 'drop') %>%
  mutate(ARREST_BORO = reorder(ARREST_BORO, -Incident_Count), # Reorder in decreasing order
         ARREST_BORO = factor(ARREST_BORO, 
                              levels = rev(levels(ARREST_BORO)),
                              labels = c("Staten Island","Manhattan","Queens","Brooklyn","Bronx"))) # Ensure factor levels are in decreasing order


ggplot(assault_3_by_borough, aes(x = ARREST_BORO, y = Incident_Count, fill = ARREST_BORO)) +
  geom_bar(stat = "identity", width = 0.5, position = "dodge") + # Make bars thinner and horizontal
  coord_flip() +  # Flip coordinates for horizontal bars
  theme_minimal() +
  labs(y = 'Borough', x = 'Number of Assault 3 Incidents', title = 'Assault 3 Incidents by Borough') +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "none") 
```

The graph presents a horizontal bar chart of Assault 3 incidents by
borough in New York City, showing Bronx with the highest number of
incidents followed by the Brooklyn, Queens, Manhattan, and Staten Island
with the fewest.

In conclusion, by analyzing factors like region, age, and gender, we can
establish a general distribution pattern for Level 3 Assault crimes.
Subsequently, we will employ regression models to discern whether these
factors are correlated with or causative of variations in crime rates.
Now, we are moving to the EDA of environmental factors.

## Environmental Factors

### Temperature

```{r echo=FALSE, message=FALSE, warning= FALSE}
ds <- readRDS("dataset/load_and_clean_data.rds")
ds <- as.data.table(ds)
ds$ARREST_DATE <- ymd(ds$ARREST_DATE)
ds$ARREST_month <- month(ds$ARREST_DATE)

ds_sub_winter <- ds[OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES"&ARREST_month %in%c(1,2,12) ,]
ds_sub_summer <- ds[OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES"&ARREST_month %in%c(6,7,8) ,]
ds_sub_spring <-ds[OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES"& ARREST_month %in%c(3,4,5) ,]
ds_sub_fall <- ds[OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES"&ARREST_month %in%c(9,10,11) ,]

ds_plot_winter <- ds_sub_winter[,.(count = .N),ARREST_PRECINCT ]
ds_plot_summer <-ds_sub_summer[,.(count = .N),ARREST_PRECINCT ]
ds_plot_spring <-ds_sub_spring[,.(count = .N),ARREST_PRECINCT ]
ds_plot_fall <-ds_sub_fall[,.(count = .N),ARREST_PRECINCT ]

map_data <- st_read("dataset/geo_export_87553334-fdcd-4d0c-8ab2-43a5e2975733.shp")
map_data <-as(map_data, "Spatial")
map_data_winter <- sp::merge(map_data,ds_plot_winter,by.x = "precinct",by.y ="ARREST_PRECINCT",all.x = T)
map_data_summer <- sp::merge(map_data,ds_plot_summer,by.x = "precinct",by.y ="ARREST_PRECINCT",all.x = T)

map_data_spring <- sp::merge(map_data,ds_plot_spring,by.x = "precinct",by.y ="ARREST_PRECINCT",all.x = T)
map_data_fall <- sp::merge(map_data,ds_plot_fall,by.x = "precinct",by.y ="ARREST_PRECINCT",all.x = T)

map_data_winter <- as(map_data_winter, "sf")
map_data_summer <-as(map_data_summer, "sf")
map_data_spring <-as(map_data_spring , "sf")
map_data_fall <- as(map_data_fall , "sf")

```

```{r echo=FALSE, message=FALSE, warning= FALSE}
map_data_winter$count_new<-cut(map_data_winter$count
                            ,breaks=c(-Inf,50,100,150,200,250,300,Inf)
                            ,labels = c(
                              "[0,50 )"
                              ,"[50,100)"
                              ,"[100,150)"
                              ,"[150,200)"
                              ,"[200,250)"
                              ,"[250,300)"
                              ,"[300+)"
                              
                              
                            ))

winter <-ggplot()+
  geom_sf(data = map_data_winter,colour = "gray60",size = 0.1,aes(fill = count_new) )+
  # scale_fill_continuous(low = "#FF9797", high = "#600000",na.value="white")+
  scale_fill_brewer(palette = "YlOrRd")+
  #scale_fill_manual(palette = "YlOrRd")+
  theme( axis.ticks=element_blank(), 
         axis.title=element_blank(), 
         panel.border = element_blank(), 
         axis.text=element_blank() , 
         panel.background = element_blank()
         
  )+ggtitle("Winter")+
  theme(plot.title = element_text(hjust = 0.5))+
  labs(fill = "Count") 
```

```{r echo=FALSE, message=FALSE, warning= FALSE}
map_data_summer$count_new<-cut(map_data_summer$count
                            ,breaks=c(-Inf,50,100,150,200,250,300,Inf)
                            ,labels = c(
                              "[0,50 )"
                              ,"[50,100)"
                              ,"[100,150)"
                              ,"[150,200)"
                              ,"[200,250)"
                              ,"[250,300)"
                              ,"[300+)"
                              
                            ))

summer<-ggplot()+
  geom_sf(data = map_data_summer,colour = "gray60",size = 0.1,aes(fill = count_new) )+
  # scale_fill_continuous(low = "#FF9797", high = "#600000",na.value="white")+
  scale_fill_brewer(palette = "YlOrRd")+
  #scale_fill_manual(palette = "YlOrRd")+
  theme( axis.ticks=element_blank(), 
         axis.title=element_blank(), 
         panel.border = element_blank(), 
         axis.text=element_blank() , 
         panel.background = element_blank()
         
  )+ggtitle("Summer")+
  theme(plot.title = element_text(hjust = 0.5))+
  labs(fill = "Count") 
```

```{r echo=FALSE, message=FALSE, warning= FALSE}
map_data_spring$count_new<-cut(map_data_spring$count
                            ,breaks=c(-Inf,50,100,150,200,250,300,Inf)
                            ,labels = c(
                              "[0,50 )"
                              ,"[50,100)"
                              ,"[100,150)"
                              ,"[150,200)"
                              ,"[200,250)"
                              ,"[250,300)"
                              ,"[300+)"
                              
                            ))

spring<-ggplot()+
  geom_sf(data = map_data_spring,colour = "gray60",size = 0.1,aes(fill = count_new) )+
  # scale_fill_continuous(low = "#FF9797", high = "#600000",na.value="white")+
  scale_fill_brewer(palette = "YlOrRd")+
  #scale_fill_manual(palette = "YlOrRd")+
  theme( axis.ticks=element_blank(), 
         axis.title=element_blank(), 
         panel.border = element_blank(), 
         axis.text=element_blank() , 
         panel.background = element_blank()
         
  )+ggtitle("Spring")+
  theme(plot.title = element_text(hjust = 0.5))+
  labs(fill = "Count") 
```

```{r echo=FALSE, message=FALSE, warning= FALSE}
map_data_fall$count_new<-cut(map_data_fall$count
                            ,breaks=c(-Inf,50,100,150,200,250,300,Inf)
                            ,labels = c(
                              "[0,50 )"
                              ,"[50,100)"
                              ,"[100,150)"
                              ,"[150,200)"
                              ,"[200,250)"
                              ,"[250,300)"
                              ,"[300+)"
                              
                            ))

fall<-ggplot()+
  geom_sf(data = map_data_fall,colour = "gray60",size = 0.1,aes(fill = count_new) )+
  # scale_fill_continuous(low = "#FF9797", high = "#600000",na.value="white")+
  scale_fill_brewer(palette = "YlOrRd")+
  #scale_fill_manual(palette = "YlOrRd")+
  theme( axis.ticks=element_blank(), 
         axis.title=element_blank(), 
         panel.border = element_blank(), 
         axis.text=element_blank() , 
         panel.background = element_blank()
         
  )+ggtitle("Fall")+
  theme(plot.title = element_text(hjust = 0.5))+ labs(fill = "Count") 
```

```{r echo=FALSE, message=FALSE, warning= FALSE}
grid.arrange(spring, fall, summer, winter, ncol = 2, nrow = 2)
```

These maps reveal a clear pattern in "Assault 3" incidents: they peak
during the summer months, likely due to the increase in outdoor
activities and social interactions that warmer weather encourages.
Spring and Fall act as transitional periods with moderate crime rates,
suggesting a correlation with the gradual changes in weather and
daylight hours, which influence how much time people spend outside. The
winter months show a marked decrease in crime rates, possibly due to the
cold weather discouraging outdoor activity. These trends indicate that
environmental factors, such as temperature and seasonality, may play a
significant role in the fluctuation of crime rates, necessitating
seasonally adjusted strategies for law enforcement and community safety
programs.

### Weather Condition

```{r echo=FALSE, message=FALSE, warning= FALSE}
# Filter for ASSAULT 3 and count occurrences per category in the 'conditions' column
assault_conditions_counts <- ds %>%
  filter(OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES") %>%
  group_by(conditions) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))
# Print the counts
#print(assault_conditions_counts)

ggplot(assault_conditions_counts, aes(x = reorder(conditions, Count), y = Count, fill = conditions)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Assault 3 Incident count by Weather Condition",
       x = " ",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The bar chart illustrates the frequency of Assault 3 incidents occurring
under various weather conditions. Clear weather and Rain with partially
is associated with the highest number of incidents, followed closely by
overcast and partially cloudy conditions. Incidents are less frequent
during more adverse weather conditions, such as combinations of snow,
rain, and overcast skies, suggesting possible correlations between
weather and the occurrence of Assault 3 crimes.

### Windspeed

```{r echo=FALSE, message=FALSE, warning= FALSE}
assault_wind_counts <- ds %>%
  filter(OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES") %>%
  mutate(WindSpeedCategory = cut(windspeed, 
                                 breaks = c(0, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50),  
                                 labels = c("(0,9]", "(9,10]", "(10,15]", "(15,20]", "(20,25]", 
                                            "(25,30]", "(30,35]", "(35,40]", "(40,45]", "(45,50]"),
                                 include.lowest = TRUE)) %>%
  group_by(WindSpeedCategory) %>%
  summarise(Count = n()) %>%
  ungroup()

ggplot(assault_wind_counts, aes(x = WindSpeedCategory, y = Count, fill = WindSpeedCategory)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = " Assault 3 incidents count by Wind Speed ", 
       x = "Wind Speed Category", 
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

This bar chart shows that incidents are most frequent in wind speeds
ranging from 10 to 20 km/h and significantly less frequent as wind
speeds increase beyond 20 km/h, suggesting fewer Assault 3 incidents
occur in higher wind conditions.

## Model

### Logistic Regression Model 

```{r echo=FALSE, message=FALSE, warning= FALSE}
ds$y <- ifelse(ds$OFNS_DESC == "ASSAULT 3 & RELATED OFFENSES",1,0)
ds$PD_DESC <- NULL
ds$OFNS_DESC  <- NULL
ds$`New Georeferenced Column` <- NULL
ds$ARREST_PRECINCT <- as.factor(ds$ARREST_PRECINCT)

set.seed(123)
size <- round(dim(ds)[1]*0.8)
index <- sample(1:dim(ds)[1],size ,replace = F)
data_train <- ds[index,]
data_valid <- ds[-index,]

```

To identify patterns in ASSAULT 3 by considering various factors such as age, gender, race of the perpetrator, as well as environmental factors like temperature and weather conditions, we decide to build a logistic regression model to predict the likelihood of an arrest being for "ASSAULT 3 & RELATED OFFENSES". This helps police understand under what circumstances such crimes are more likely to occur, aiding in the development of proactive policing strategies.

first,we use a binary response variable y is created to represent whether each arrest falls into this category. The data is then split into training and validation sets, we randomly select 80% to be training set, 20% to be validation set.

### Logisitic Model result
```{r echo=FALSE, message=FALSE, warning= FALSE}
model_lr <- glm(y ~ .
                , family = binomial
                , data = data_train)
```

```{r include=FALSE}
summary(model_lr)
```

From the summary of the model, 


# Confusion Matrix for log model
```{r  echo=FALSE, message=FALSE, warning= FALSE}
library(MLmetrics)
predict_lr <- predict(model_lr,data_valid,type = "response")
predict_classes <- ifelse(predict_lr > 0.5, 1, 0)
```

```{r  echo=FALSE, message=FALSE, warning= FALSE}

conf_matrix <- table(Predicted = predict_classes, Actual = data_valid$y)

print(conf_matrix)

accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", accuracy))
print(paste("AUC:", AUC(predict_lr,data_valid$y)))
```

To better evaluate the model, We build a confusion matrix to show the accuracy of the model, which is the proportion of correct predictions (true positives and true negatives) out of the total predictions, is calculated at approximately 84.88%. This indicates that the model performs well in general. Additionally, the Area Under the Curve (AUC) score of 0.8222, shown below the confusion matrix, further confirms the model's effectiveness in distinguishing between the two classes. 

```{r  echo=FALSE, message=FALSE, warning= FALSE}
data_valid[which(predict_lr>0.70), ]
```

We filtered out cases where the logistic regression model predicted a probability greater than 0.70 for the outcome of interest, focusing on instances where the model confidently predicted an arrest as "ASSAULT 3."

The results indicate that "ASSAULT 3" crimes typically occur between March and December, especially during rainy or cloudy weather with temperatures above zero Celsius. These incidents predominantly happen in central Brooklyn, the Bronx, and uptown Manhattan. At the precinct level, Precincts 67, 32, and 42 are notable hotspots.

Demographically, the majority of offenders are African American or White Hispanic, often around or below 18 years of age. Interestingly, the data also shows a significant proportion of female offenders, primarily involved in misdemeanor-level offenses.



## Decision Trees model
```{r  echo=FALSE, message=FALSE, warning= FALSE}

train_dummy <- model.matrix(~ LAW_CAT_CD
                            +ARREST_BORO+ARREST_PRECINCT
                            +AGE_GROUP + PERP_SEX
                            +PERP_RACE +conditions            
                            - 1
                            , data = data_train)
train_all <- data.frame(train_dummy
                        , ARREST_DATE  = data_train$ARREST_DATE
                        , temp = data_train$temp
                        , windspeed = data_train$windspeed
                        , cloudcover = data_train$cloudcover
                       ,y = data_train$y
                        )
```


```{r  echo=FALSE, message=FALSE, warning= FALSE}

valid_dummy <- model.matrix(~ LAW_CAT_CD
                            +ARREST_BORO+ARREST_PRECINCT
                            +AGE_GROUP + PERP_SEX
                            +PERP_RACE +conditions            
                            - 1
                            , data = data_valid)
valid_all <- data.frame(valid_dummy
                        , ARREST_DATE  = data_valid$ARREST_DATE
                        , temp = data_valid$temp
                        , windspeed = data_valid$windspeed
                        , cloudcover = data_valid$cloudcover
                        ,y = data_valid$y
                        )



```

Next we decide to use Decision tree model to predict whether an arrest falls into the category of "ASSAULT 3" based on various demographic, legal, and environmental factors.It will provides a clear framework for understanding and predicting "ASSAULT 3", helping law enforcement and policymakers make informed decisions based on data-driven insights.



```{r  echo=FALSE, message=FALSE, warning= FALSE}
model_tree <- rpart(y ~ .
                    , data = train_all, method = "class"
                    ,control=rpart.control(minsplit=1, minbucket=1, cp=0.0003))
rpart.plot(model_tree,cex=0.35,type = 3) ###cex=0.35,type = 3
```

```{r  incldue=FALSE}
summary(model_tree)
```


#Confusion Matrix 
```{r  echo=FALSE, message=FALSE, warning= FALSE}
predict_tree <- predict(model_tree,valid_all,type = "prob")[,2]
predict_classes_tree <- ifelse(predict_tree > 0.5, 1, 0)
```



```{r  echo=FALSE, message=FALSE, warning= FALSE}

conf_matrix_tree <- table(Predicted = predict_classes_tree, Actual = data_valid$y)


print(conf_matrix_tree)

accuracy_tree <- sum(diag(conf_matrix_tree)) / sum(conf_matrix_tree)
print(paste("Accuracy:", accuracy_tree))
print(paste("AUC:", AUC(predict_tree,data_valid$y)))
```

To better evaluate the model, We build a confusion matrix to show the accuracy of the model, which is the proportion of correct predictions (true positives and true negatives) out of the total predictions, is calculated at approximately 84.86%. This indicates that the model performs well in general. Additionally, the Area Under the Curve (AUC) score of 0.7897, shown below the confusion matrix, further confirms the model's effectiveness in distinguishing between the two classes. 



## Compare the Accuracy between these two models
```{r  include=FALSE}
library(pROC)
roc_lr <- roc(data_valid$y,predict_lr)
roc_tree <- roc(data_valid$y,predict_tree)

```


```{r  echo=FALSE, message=FALSE, warning= FALSE}
roc_df <- data.frame(
  Sensitivity = c(0, roc_lr$sensitivities),
  OneMinusSpecificity = c(0, 1 - roc_lr$specificities),
  Model = rep("Logistic", length(roc_lr$sensitivities) + 1)
)

roc_df <- rbind(roc_df,
                data.frame(
                  Sensitivity = c(0, roc_tree$sensitivities),
                  OneMinusSpecificity = c(0, 1 - roc_tree$specificities),
                  Model = rep("Tree", length(roc_tree$sensitivities) + 1)
                ))

library(ggplot2)

auc_plot <- ggplot(roc_df, aes(x = OneMinusSpecificity, y = Sensitivity, color = Model)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "AUC Comparison", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()
auc_plot
```

We use the ROC curves to evaluate the performance of classification models. The area under the curve (AUC) for each model can be used to quantify the overall performance. The higher the AUC, the better the model's ability to discriminate between the two classes.The logistic model (red curve) appears to outperform the tree model (blue curve) by achieving a higher AUC, which indicates that it has a better overall classification performance for the arrest data. 

