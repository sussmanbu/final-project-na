---
title: Data
toc: true
draft: false
---
![](images/general-view-nypd-arm-patch-26440914.webp)


## Data Background
### Data source and reasons to collect it

This datasets comes from the [NYC Opendata](https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u/about_data) and is collected by the Office of Management Analysis and Planning of the New York City Police Department. This dataset provides a general overview of individuals arrested by the NYPD in 2023. 

The NYPD's initiative to collect and disseminate data on crime rates and police enforcement activities is a multifaceted strategy aimed at enhancing public safety, ensuring transparency, and fostering community trust. This effort not only serves to keep the public informed about the safety of their neighborhoods but also aids the police department in strategic planning and efficient resource allocation. By analyzing crime trends, the NYPD can deploy resources more effectively, targeting areas with higher crime rates or specific types of criminal activities. Furthermore, making such data accessible supports policy making and evaluation, enabling lawmakers to craft and assess laws and policies with an eye toward crime reduction. Additionally, this transparency encourages community engagement and collaboration, as residents become more aware of the challenges their communities face and are thus more likely to work alongside the police to develop strategies for crime prevention. In sum, the NYPD's collection and sharing of this dataset are integral to its broader objectives of improving public safety, enhancing operational efficiency, and building a collaborative relationship with the communities it serves.


### Issues on how the data was collected  

The collection of NYPD Arrest Data raises several concerns regarding its collection and subsequent analysis. Reporting biases might result from inconsistencies in how incidents are documented across different precincts or by individual officers, affecting data uniformity and reliability. Human error in data entry could introduce inaccuracies, including misclassifications or incorrect demographic information, which could mislead analyses, particularly in trend or demographic studies.

### Sample's biases
There might be some biases in the sample. In the dataset, certain age groups are disproportionately represented.Over representation of the 25-44 age group and certain racial demographics suggests possible targeted enforcement or societal issues, which could bias data-driven analyses. Additionally, the dataset disproportionately represents certain demographics in terms of race.

### The purpose about the data

The NYPD's data collection and dissemination initiative aims to improve public safety, enhance transparency, and build community trust. By providing crime rate and police activity data, the NYPD can better allocate resources, target high-crime areas, and support informed policymaking. This transparency also promotes community engagement, empowering residents to collaborate with the police in developing crime prevention strategies. Overall, this initiative supports the NYPD's goals of enhancing safety, operational efficiency, and community collaboration.

## Data Loading, Merging,and Cleaning

### Merging Datasets

In our research, in addition to the explantory analysis of arrest data, we wanted to take it a step further and look at the non-human factors that affect crime, so we added the daily weather conditions of New York City over the course of a year to our master data. This data comes from [Visual crossing](https://www.visualcrossing.com/weather/weather-data-services#)'s weather database.

We selected basic weather conditions for New York City for the year 2023 so that we could explore whether extremes in temperature, wind speed, etc, affect crime rates, as well as crime trends under different weather conditions. This way it can be more efficient to optimize the efficiency of NYPD's police response. 

The merging process codes will shows in following load and cleaning script.


```{r include=F}
library(tidyverse) 
library(ggplot2)
library(dplyr)
library(kableExtra)
library(data.table)
library(magrittr)
library(knitr)
```

[Data Loading and Cleaning Script](scripts/load_and_clean_data.R)

In the cleaning of the crime dataset, we delete the arrest ID, two police classification ID, jurisdiction code, and four coordinate variables . These variables are redundant and not relevant with our research. Then, We remove rows with any NA and null values from the dataset. We also delete undefined data in Level of offense: felony, misdemeanor, violation, infraction, besides that, we deleted the race recorded as unknown.

In the cleaning of the weather dataset, we just keep the date, daily temperature, windspeed, and keywords of daily weather conditions. following the same steps, we clean all the missing value and merge it with the arrest dataset. We merging two datasets based on their date columns, ensuring that all entries from the weather dataset are included and correspond with the crime dataset

All in all, it have 226244 rows and 13 columns. Below part shows the modified variables table. 

## Variables Description

```{r echo=FALSE}
ds <- read_rds("dataset/load_and_clean_data.rds") 
```


```{r echo =F}

variables_df <- data.frame(
  Variable = c("ARREST_DATE",  "PD_DESC",  "OFNS_DESC", "LAW_CAT_CD",
               "ARREST_BORO",
              "ARREST_PRECINCT","AGE_GROUP", "PERP_SEX", "PERP_RACE", 
              "temp","windspeed","conditons"),
  Data_Types = c("calendar date", "text", "text", 
                 "text","text",
                 "number","text", "text", "text", 
                 "number","number","text"),
  Description = c( 
                  "Exact date of arrest for the reported event", 
                  "Description of internal classification corresponding with PD code (more granular than Offense Description)", 
                  "Description of internal classification corresponding with KY code (more general category than PD description)", 
                  "Level of offense: felony, misdemeanor, violation, infraction", 
                  "	Borough of arrest. B(Bronx), S(Staten Island), K(Brooklyn), M(Manhattan), Q(Queens)",
                  
                  "Precinct where the arrest occurred", 
                  "Perpetrator’s age within a category", 
                  "Perpetrator’s sex description", 
                  "Perpetrator’s race description", 
                  "Average Daily temperature in Celsius",
                  "Daily Windspeed in Kilometer",
                  "Short description about the weather")
)
kable(variables_df, format = "html")|>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = F, 
                font_size = 12) |>
  column_spec(1, bold = T, color = "black") |>
  row_spec(0, background = "#D3D3D3") 
```

## Top 10 crime categories description

There are 62 crimes, the below table shows the specific description and level of the top 10 crimes in order of the incident number.

```{r echo=FALSE}
top_categories <- ds %>%
  count(OFNS_DESC, sort = TRUE) %>%
  top_n(10, wt = n)

# Join the top categories back to the original dataset to filter and preserve the order
filtered_ds <- ds %>%
  inner_join(top_categories, by = "OFNS_DESC")

# Summarise the details for these top categories without rearranging them alphabetically
category_details <- filtered_ds |>
  group_by(OFNS_DESC) |>
  summarise(PD_DESC_Details = paste(unique(PD_DESC), collapse = ", "), .groups = "drop") %>%
  arrange(match(OFNS_DESC, top_categories$OFNS_DESC))  # Order by frequency

# Generate the table with kable and kableExtra
category_details |>
  kable(format = "html", escape = F) |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = F, 
                font_size = 12) |>
  column_spec(1, bold = T, color = "black") |>
  row_spec(0, background = "#D3D3D3") 
```

### R packages
These are the following new packages were used in making this project.
kableExtra
data.table
magrittr
knitr
install.packages("corrplot")
library(MLmetrics)


